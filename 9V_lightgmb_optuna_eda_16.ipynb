{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carolima/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import json\n",
    "import joblib \n",
    "import os\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset after the exploratory data analysis\n",
    "challenge_set_updated = pd.read_csv(\"./data/challenge_set_updated_v16.csv\")\n",
    "submission_set = pd.read_csv(\"./data/submission_set.csv\")\n",
    "submission_set_updated = pd.read_csv(\"./data/submission_set_updated_v16.csv\")\n",
    "\n",
    "# If necessary change this part to test the model before the training process\n",
    "df = challenge_set_updated.iloc[:,:]\n",
    "# df = challenge_set_updated.sample(frac=0.001)\n",
    "\n",
    "# Separating features and target variable\n",
    "X = df.drop('tow', axis=1)\n",
    "y = df['tow']\n",
    "\n",
    "n_jobs = os.cpu_count() // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "categorical_columns = ['adep', 'ades', 'aircraft_type', 'wtc', 'airline', 'offblock_season',\n",
    "                       'flight_duration_category', 'adep_region', 'ades_region',\n",
    "                       'flight_direction', 'Manufacturer', 'Model_FAA',\n",
    "                       'Physical_Class_Engine', 'FAA_Weight']\n",
    "\n",
    "# Encoding using LabelEncoder\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))  # Ensure data is string type before encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable to hold the trained model pipeline\n",
    "global_model = None\n",
    "\n",
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    global global_model\n",
    "\n",
    "    # Hyperparameters to be tuned by Optuna\n",
    "    param = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',  # You can leave this as 'gbdt' for GPU acceleration\n",
    "        'verbosity': -1,\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'min_data_per_group': trial.suggest_int('min_data_per_group', 50, 150),\n",
    "        'cat_smooth': trial.suggest_float('cat_smooth', 5, 20),\n",
    "        'device': 'gpu'  # This enables GPU training\n",
    "    }\n",
    "\n",
    "    # Convert DataFrame to LightGBM Dataset, specifying categorical features\n",
    "    train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_columns, free_raw_data=False)\n",
    "    valid_data = lgb.Dataset(X_val, label=y_val, categorical_feature=categorical_columns, free_raw_data=False)\n",
    "\n",
    "    # Train the model with early stopping\n",
    "    model = lgb.train(\n",
    "        param,\n",
    "        train_data,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[valid_data],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=50)]\n",
    "    )\n",
    "\n",
    "    # Predictions on the validation set\n",
    "    preds = model.predict(X_val)\n",
    "    rmse = mean_squared_error(y_val, preds, squared=False)  # Calculate RMSE\n",
    "\n",
    "    # Save the trained model pipeline globally\n",
    "    global_model = model\n",
    "\n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-16 13:17:38,959] A new study created in memory with name: no-name-0fbebc6f-fb3a-4482-b792-a02f31441557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[997]\tvalid_0's rmse: 2606.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carolima/.local/lib/python3.10/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "[I 2024-10-16 14:41:09,624] Trial 0 finished with value: 2606.6094999350394 and parameters: {'num_leaves': 26, 'learning_rate': 0.18763891802032998, 'min_child_samples': 72, 'min_data_per_group': 139, 'cat_smooth': 7.573846384937663}. Best is trial 0 with value: 2606.6094999350394.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[998]\tvalid_0's rmse: 2564.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carolima/.local/lib/python3.10/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "[I 2024-10-16 18:07:13,804] Trial 1 finished with value: 2564.214247256467 and parameters: {'num_leaves': 74, 'learning_rate': 0.13409243208928043, 'min_child_samples': 5, 'min_data_per_group': 85, 'cat_smooth': 9.750015618641832}. Best is trial 1 with value: 2564.214247256467.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's rmse: 2546.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carolima/.local/lib/python3.10/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "[I 2024-10-16 22:18:26,438] Trial 2 finished with value: 2546.4921435437986 and parameters: {'num_leaves': 96, 'learning_rate': 0.06256188395638097, 'min_child_samples': 63, 'min_data_per_group': 110, 'cat_smooth': 8.757471384663825}. Best is trial 2 with value: 2546.4921435437986.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "{'num_leaves': 96, 'learning_rate': 0.06256188395638097, 'min_child_samples': 63, 'min_data_per_group': 110, 'cat_smooth': 8.757471384663825}\n",
      "Best RMSE: 2546.4921435437986\n"
     ]
    }
   ],
   "source": [
    "# Create a study object and specify the direction of the optimization\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=3)  # Specify the number of trials\n",
    "\n",
    "# Best trial results\n",
    "print(\"Best trial:\")\n",
    "print(study.best_trial.params)\n",
    "print(\"Best RMSE:\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_params = study.best_trial.params\n",
    "\n",
    "# Train the final model on the full training set using the best parameters\n",
    "best_params.update({\n",
    "    'metric': 'rmse',\n",
    "    'verbosity': -1,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression'\n",
    "})\n",
    "bst = lgb.train(\n",
    "    best_params,\n",
    "    lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_columns, free_raw_data=False),\n",
    "    num_boost_round=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        flight_id        date                          callsign  adep  \\\n",
      "0       248753821  2022-01-01  3b3de0f3ad0ee192513995c02f7bf7cf  LTFJ   \n",
      "1       248753822  2022-01-01  e06dd03d4a879ca37d9e18c1bd7cad16  EBBR   \n",
      "2       248754498  2022-01-01  2d3b1c962c78c4ebeef11bcd51b9e94c  KMIA   \n",
      "3       248757623  2022-01-01  81564432d3ee97c4bdf4cd8f006753dc  EGCN   \n",
      "4       248763603  2022-01-01  84be079d7e660db105d91f600b4b3d59  EIDW   \n",
      "...           ...         ...                               ...   ...   \n",
      "105954  258066302  2022-12-31  2d3b4446c4d05a25196a9d52cab936fb  LTFJ   \n",
      "105955  258068609  2022-12-31  253fd692ed441fac523081471c067772  LOWW   \n",
      "105956  258068876  2022-12-31  c9fca302ca2e28acab0eb0bb1b46f11b  LTFM   \n",
      "105957  258064675  2022-12-31  00f96ad0e382476649574ba044c764fc  EHAM   \n",
      "105958  258058370  2022-12-31  5f0c222c7f7ceff3fbe75c854cce74c9  UBBB   \n",
      "\n",
      "                     name_adep country_code_adep  ades          name_ades  \\\n",
      "0       Istanbul Sabiha Gokcen                TR  LFLL               Lyon   \n",
      "1                     Brussels                BE  KJFK       New York JFK   \n",
      "2                        Miami                US  EGLL    London Heathrow   \n",
      "3          Doncaster Sheffield                GB  LEAL           Alicante   \n",
      "4                       Dublin                IE  LFLL               Lyon   \n",
      "...                        ...               ...   ...                ...   \n",
      "105954  Istanbul Sabiha Gokcen                TR  EKCH         Copenhagen   \n",
      "105955                  Vienna                AT  KIAD  Washington Dulles   \n",
      "105956            iGA Istanbul                TR  LSZH             Zurich   \n",
      "105957               Amsterdam                NL  EDDF          Frankfurt   \n",
      "105958                    Baku                AZ  LTFM       iGA Istanbul   \n",
      "\n",
      "       country_code_ades  actual_offblock_time          arrival_time  \\\n",
      "0                     FR  2022-01-01T09:44:00Z  2022-01-01T12:48:33Z   \n",
      "1                     US  2022-01-01T09:45:00Z  2022-01-01T17:49:51Z   \n",
      "2                     GB  2022-01-01T01:52:00Z  2022-01-01T09:55:16Z   \n",
      "3                     ES  2022-01-01T08:20:00Z  2022-01-01T11:06:08Z   \n",
      "4                     FR  2022-01-01T11:01:00Z  2022-01-01T13:00:43Z   \n",
      "...                  ...                   ...                   ...   \n",
      "105954                DK  2022-12-31T09:36:00Z  2022-12-31T13:12:17Z   \n",
      "105955                US  2022-12-31T09:49:00Z  2022-12-31T19:38:26Z   \n",
      "105956                CH  2022-12-31T09:25:00Z  2022-12-31T12:24:24Z   \n",
      "105957                DE  2022-12-31T10:04:21Z  2022-12-31T10:55:35Z   \n",
      "105958                TR  2022-12-31T09:40:00Z  2022-12-31T12:28:00Z   \n",
      "\n",
      "       aircraft_type wtc                           airline  flight_duration  \\\n",
      "0               B738   M  6351ec1b849adacc0cbb3b1313d8d39b              170   \n",
      "1               A333   H  bdeeef3a675587d530de70a25d7118d2              470   \n",
      "2               B77W   H  5543e4dc327359ffaf5b9c0e6faaf0e1              473   \n",
      "3               B38M   M  3922524069809ac4326134429751e26f              156   \n",
      "4               A320   M  a73f82288988b79be490c6322f4c32ed              105   \n",
      "...              ...  ..                               ...              ...   \n",
      "105954          B38M   M  6351ec1b849adacc0cbb3b1313d8d39b              201   \n",
      "105955          B763   H  5d407cb11cc29578cc3e292e743f5393              575   \n",
      "105956          A321   M  6351ec1b849adacc0cbb3b1313d8d39b              154   \n",
      "105957          A320   M  f502877cab405652cf0dd70c2213e730               42   \n",
      "105958          B738   M  6351ec1b849adacc0cbb3b1313d8d39b              158   \n",
      "\n",
      "        taxiout_time  flown_distance            tow  \n",
      "0                 15            1122   70513.271458  \n",
      "1                 15            3205  204459.518007  \n",
      "2                 10            3965  222227.451944  \n",
      "3                 10             986   55060.293908  \n",
      "4                 15             686   66305.054125  \n",
      "...              ...             ...            ...  \n",
      "105954            15            1199   67853.017877  \n",
      "105955            14            3937  162147.274725  \n",
      "105956            25             988   73104.253696  \n",
      "105957             9             240   58232.581164  \n",
      "105958            10            1014   65105.787365  \n",
      "\n",
      "[105959 rows x 18 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# List of columns to encode\n",
    "categorical_columns = ['adep', 'ades', 'aircraft_type', 'wtc', 'airline', 'offblock_season', 'flight_duration_category', \n",
    "                       'adep_region', 'ades_region', 'flight_direction', 'Manufacturer', 'Model_FAA', \n",
    "                       'Physical_Class_Engine', 'FAA_Weight']\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoders = {}\n",
    "\n",
    "\n",
    "# Apply Label Encoding to each categorical column100aa\n",
    "for column in categorical_columns:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    submission_set_updated[column] = label_encoders[column].fit_transform(submission_set_updated[column])\n",
    "\n",
    "\n",
    "# Now you should be able to run LightGBM predictions\n",
    "submission_set_features = submission_set_updated.iloc[:, :-1]\n",
    "submission_set['tow'] = bst.predict(submission_set_features)\n",
    "\n",
    "print(submission_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to submissions/submission_20241017_005132.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the submissions directory and create it if it doesn't exist\n",
    "submissions_dir = 'submissions'\n",
    "os.makedirs(submissions_dir, exist_ok=True)\n",
    "\n",
    "# Define a timestamp for the file name\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save the submission with a timestamp in the filename\n",
    "submission_file = os.path.join(submissions_dir, f\"submission_{timestamp}.csv\")\n",
    "\n",
    "# Assuming submission_set is a DataFrame, save it to CSV\n",
    "submission_set.to_csv(submission_file, index=False)\n",
    "\n",
    "print(f\"Submission saved to {submission_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
