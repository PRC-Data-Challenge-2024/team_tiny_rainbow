{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed357e88-fc3e-40ea-8e09-381b3ee88af5",
   "metadata": {},
   "source": [
    "# Saint Model Architecture\n",
    "\n",
    "Code from https://github.com/somepago/saint adapted for the dataset and notebook execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8f1cd4-0b63-421a-b114-1bccb6355c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from saint.pretrainmodel import SAINT\n",
    "from saint.data_openml import data_prep_openml, task_dset_ids, DataSetCatCon\n",
    "from saint.pretraining import SAINT_pretrain\n",
    "import argparse\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from saint.utils import count_parameters, classification_scores, mean_sq_error, get_scheduler\n",
    "from saint.augmentations import embed_data_mask, add_noise\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd0495f-9fa4-4fd4-bc96-f04b3974d6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SimpleNamespace(**{\n",
    "  'dset_id': 'v8',  # dataset version\n",
    "  'vision_dset': False,\n",
    "  'task': 'regression', 'dtask': 'reg',\n",
    "  'cont_embeddings': 'MLP', # 'MLP', 'Noemb', 'pos_singleMLP'\n",
    "  'embedding_size': 32,\n",
    "  'transformer_depth': 6,\n",
    "  'attention_heads': 8,\n",
    "  'attention_dropout': 0.1,\n",
    "  'ff_dropout': 0.1,\n",
    "  'attentiontype': 'colrow',  #  'col', 'colrow', 'row', 'justmlp', 'attn', 'attnmlp'\n",
    "  'optimizer': 'AdamW',  # 'AdamW', 'Adam', 'SGD'\n",
    "  'scheduler': 'cosine',  # 'cosine', 'linear'\n",
    "  'lr': 0.0001,\n",
    "  'epochs': 100,\n",
    "  'batchsize': 256,\n",
    "  'savemodelroot': './bestmodels',\n",
    "  'run_name': 'testrun',\n",
    "  'set_seed': 1,\n",
    "  'dset_seed': 1,\n",
    "  'active_log': True,  # wandb\n",
    "  'pretrain': True,  # test with False # TODO\n",
    "  'pretrain_epochs': 50,\n",
    "  'pt_tasks': ['contrastive', 'denoising'],  # 'contrastive', 'contrastive_sim', 'denoising'\n",
    "  'pt_aug': [],  # 'mixup', 'cutmix' (list)\n",
    "  'pt_aug_lam': 0.1,\n",
    "  'mixup_lam': 0.3,\n",
    "  'train_noise_type': 'missing', # None, 'missing', 'cutmix'\n",
    "  'train_noise_level': 0.01, \n",
    "  'ssl_samples': None,  # int or None\n",
    "  'pt_projhead_style': 'diff',  # 'diff', 'same', 'nohead'\n",
    "  'nce_temp': 0.7,\n",
    "  'lam0': 0.5,\n",
    "  'lam1': 10,\n",
    "  'lam2': 1,\n",
    "  'lam3': 10,\n",
    "  'final_mlp_style': 'sep' # 'common', 'sep'\n",
    "})\n",
    "modelsave_path = os.path.join(os.getcwd(),opt.savemodelroot,opt.task,str(opt.dset_id),opt.run_name)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device is {device}.\")\n",
    "torch.manual_seed(opt.set_seed)\n",
    "os.makedirs(modelsave_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcea470-cdb2-491a-b2b4-8230ef3a97eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if opt.active_log:\n",
    "    import wandb\n",
    "    if opt.train_noise_type is not None and opt.train_noise_level > 0:\n",
    "        wandb.init(project=\"saint_v2_robustness\", group =f'{opt.run_name}_{opt.task}' ,name = f'{opt.task}_{opt.train_noise_type}_{str(opt.train_noise_level)}_{str(opt.attentiontype)}_{str(opt.dset_id)}')\n",
    "    elif opt.ssl_samples is not None:\n",
    "        wandb.init(project=\"saint_v2_ssl\", group = f'{opt.run_name}_{opt.task}' ,name = f'{opt.task}_{str(opt.ssl_samples)}_{str(opt.attentiontype)}_{str(opt.dset_id)}')\n",
    "    else:\n",
    "        raise'wrong config.check the file you are running'\n",
    "    wandb.config.update(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f9b07b-a71f-4ea4-bbd7-7b9c5867c595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_flight_data(filepath, datasplit=[.80, .1, .1]):\n",
    "    return None\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # Convert the date columns to datetime objects\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['actual_offblock_time'] = pd.to_datetime(df['actual_offblock_time'])\n",
    "    df['arrival_time'] = pd.to_datetime(df['arrival_time'])\n",
    "\n",
    "    # Feature engineering: Extracting month, day of the week, and hour (rounded to the nearest hour)\n",
    "    df['offblock_hour'] = df['actual_offblock_time'].dt.round('H').dt.hour\n",
    "    df['offblock_day_of_week'] = df['actual_offblock_time'].dt.dayofweek\n",
    "    df['offblock_month'] = df['actual_offblock_time'].dt.month\n",
    "\n",
    "    # Convert the new features to categorical data types\n",
    "    df['offblock_hour'] = df['offblock_hour'].astype('category')\n",
    "    df['offblock_day_of_week'] = df['offblock_day_of_week'].astype('category')\n",
    "    df['offblock_month'] = df['offblock_month'].astype('category')\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    df = df.drop(columns=['flight_id','name_adep', 'callsign', 'actual_offblock_time', 'arrival_time', 'date'])\n",
    "        \n",
    "    # Define categorical and continuous columns\n",
    "    categorical_cols = ['adep', 'country_code_adep', 'ades', 'name_ades', 'country_code_ades',\n",
    "                        'aircraft_type', 'wtc', 'airline', 'offblock_hour', 'offblock_day_of_week', 'offblock_month']\n",
    "    continuous_cols = ['flight_duration', 'taxiout_time', 'flown_distance', 'tow']\n",
    "\n",
    "    # Handle missing values\n",
    "    df = df.fillna('MissingValue')\n",
    "\n",
    "    # Encode categorical columns\n",
    "    cat_dims = []\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "        cat_dims.append(df[col].nunique())\n",
    "    \n",
    "    # Normalize continuous columns\n",
    "    scaler = StandardScaler()\n",
    "    df[continuous_cols] = scaler.fit_transform(df[continuous_cols])\n",
    "    \n",
    "    # Target column (tow - Take Off Weight)\n",
    "    y = df['tow'].values\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    df = df.drop(columns=['flight_id', 'tow'])\n",
    "    \n",
    "    # Split the dataset into train, validation, and test sets\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(df, y, test_size=1 - datasplit[0], random_state=42)\n",
    "    X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=datasplit[2] / (datasplit[1] + datasplit[2]), random_state=42)\n",
    "    \n",
    "    # Define indices of categorical and continuous columns\n",
    "    cat_idxs = [X_train.columns.get_loc(col) for col in categorical_cols]\n",
    "    con_idxs = [X_train.columns.get_loc(col) for col in continuous_cols]\n",
    "    \n",
    "    # Calculate mean and std for continuous columns in the training set\n",
    "    train_mean, train_std = X_train[continuous_cols].mean().values, X_train[continuous_cols].std().values\n",
    "    train_std = np.where(train_std < 1e-6, 1e-6, train_std)\n",
    "\n",
    "    return cat_dims, cat_idxs, con_idxs, X_train, y_train, X_valid, y_valid, X_test, y_test, train_mean, train_std\n",
    "\n",
    "cat_dims, cat_idxs, con_idxs, X_train, y_train, X_valid, y_valid, X_test, y_test, train_mean, train_std = \\\n",
    "  load_flight_data(opt.dset_id, opt.dset_seed,opt.task, datasplit=[.65, .15, .2])\n",
    "continuous_mean_std = np.array([train_mean,train_std]).astype(np.float32)\n",
    "cat_dims = np.append(np.array([1]),np.array(cat_dims)).astype(int) #Appending 1 for CLS token, this is later used to generate embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124a25d3-8f52-440a-983e-4608989cc3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Setting some hyperparams based on inputs and dataset\n",
    "_,nfeat = X_train['data'].shape\n",
    "if nfeat > 100:\n",
    "    opt.embedding_size = min(4,opt.embedding_size)\n",
    "    opt.batchsize = min(64, opt.batchsize)\n",
    "if opt.attentiontype != 'col':\n",
    "    opt.transformer_depth = 1\n",
    "    opt.attention_heads = 4\n",
    "    opt.attention_dropout = 0.8\n",
    "    opt.embedding_size = 16\n",
    "    if opt.optimizer =='SGD':\n",
    "        opt.ff_dropout = 0.4\n",
    "        opt.lr = 0.01\n",
    "    else:\n",
    "        opt.ff_dropout = 0.8\n",
    "\n",
    "opt.batchsize = 2048 # TODO optimize\n",
    "\n",
    "nfeat, opt.batchsize, opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e58c71-522d-4f39-9efd-773463c86433",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = DataSetCatCon(X_train, y_train, cat_idxs,opt.dtask,continuous_mean_std)\n",
    "trainloader = DataLoader(train_ds, batch_size=opt.batchsize, shuffle=True,num_workers=4)\n",
    "\n",
    "valid_ds = DataSetCatCon(X_valid, y_valid, cat_idxs,opt.dtask, continuous_mean_std)\n",
    "validloader = DataLoader(valid_ds, batch_size=opt.batchsize, shuffle=False,num_workers=4)\n",
    "\n",
    "test_ds = DataSetCatCon(X_test, y_test, cat_idxs,opt.dtask, continuous_mean_std)\n",
    "testloader = DataLoader(test_ds, batch_size=opt.batchsize, shuffle=False,num_workers=4)\n",
    "\n",
    "y_dim = 1 # opt.task 'regression'\n",
    "criterion = nn.MSELoss().to(device)\n",
    "\n",
    "model = SAINT(\n",
    "    categories = tuple(cat_dims), \n",
    "    num_continuous = len(con_idxs),                \n",
    "    dim = opt.embedding_size,                           \n",
    "    dim_out = 1,                       \n",
    "    depth = opt.transformer_depth,                       \n",
    "    heads = opt.attention_heads,                         \n",
    "    attn_dropout = opt.attention_dropout,             \n",
    "    ff_dropout = opt.ff_dropout,                  \n",
    "    mlp_hidden_mults = (4, 2),       \n",
    "    cont_embeddings = opt.cont_embeddings,\n",
    "    attentiontype = opt.attentiontype,\n",
    "    final_mlp_style = opt.final_mlp_style,\n",
    "    y_dim = y_dim\n",
    ")\n",
    "vision_dset = opt.vision_dset\n",
    "\n",
    "# print(count_parameters(model))\n",
    "# import ipdb; ipdb.set_trace()\n",
    "model.to(device)\n",
    "\n",
    "if opt.pretrain:\n",
    "    model = SAINT_pretrain(model, cat_idxs,X_train,y_train, continuous_mean_std, opt,device)\n",
    "\n",
    "if opt.ssl_samples is not None and opt.ssl_samples > 0 :\n",
    "    print('We are in semi-supervised learning case')\n",
    "    train_pts_touse = np.random.choice(X_train['data'].shape[0], opt.ssl_samples)\n",
    "    X_train['data'] = X_train['data'][train_pts_touse,:]\n",
    "    y_train['data'] = y_train['data'][train_pts_touse]\n",
    "    \n",
    "    X_train['mask'] = X_train['mask'][train_pts_touse,:]\n",
    "    train_bsize = min(opt.ssl_samples//4,opt.batchsize)\n",
    "\n",
    "    train_ds = DataSetCatCon(X_train, y_train, cat_idxs,opt.dtask,continuous_mean_std)\n",
    "    trainloader = DataLoader(train_ds, batch_size=train_bsize, shuffle=True,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcf2981-de13-4a56-bf3d-73c0920df6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt.optimizer == 'SGD':\n",
    "    optimizer = optim.SGD(model.parameters(), lr=opt.lr,\n",
    "                          momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = get_scheduler(opt, optimizer)\n",
    "elif opt.optimizer == 'Adam':\n",
    "    optimizer = optim.Adam(model.parameters(),lr=opt.lr)\n",
    "elif opt.optimizer == 'AdamW':\n",
    "    optimizer = optim.AdamW(model.parameters(),lr=opt.lr)\n",
    "best_valid_auroc = 0\n",
    "best_valid_accuracy = 0\n",
    "best_test_auroc = 0\n",
    "best_test_accuracy = 0\n",
    "best_valid_rmse = 100000\n",
    "\n",
    "for epoch in range(opt.epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "        # x_categ is the the categorical data, with y appended as last feature. x_cont has continuous data. cat_mask is an array of ones same shape as x_categ except for last column(corresponding to y's) set to 0s. con_mask is an array of ones same shape as x_cont. \n",
    "        x_categ, x_cont, y_gts, cat_mask, con_mask = data[0].to(device), data[1].to(device),data[2].to(device),data[3].to(device),data[4].to(device)\n",
    "        if opt.train_noise_type is not None and opt.train_noise_level>0:\n",
    "            noise_dict = {\n",
    "                'noise_type' : opt.train_noise_type,\n",
    "                'lambda' : opt.train_noise_level\n",
    "            }\n",
    "            if opt.train_noise_type == 'cutmix':\n",
    "                x_categ, x_cont = add_noise(x_categ,x_cont, noise_params = noise_dict)\n",
    "            elif opt.train_noise_type == 'missing':\n",
    "                cat_mask, con_mask = add_noise(cat_mask, con_mask, noise_params = noise_dict)\n",
    "        # We are converting the data to embeddings in the next step\n",
    "        _ , x_categ_enc, x_cont_enc = embed_data_mask(x_categ, x_cont, cat_mask, con_mask,model,vision_dset)           \n",
    "        reps = model.transformer(x_categ_enc, x_cont_enc)\n",
    "        # select only the representations corresponding to y and apply mlp on it in the next step to get the predictions.\n",
    "        y_reps = reps[:,0,:]\n",
    "        \n",
    "        y_outs = model.mlpfory(y_reps)\n",
    "        if opt.task == 'regression':\n",
    "            loss = criterion(y_outs,y_gts) \n",
    "        else:\n",
    "            loss = criterion(y_outs,y_gts.squeeze()) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if opt.optimizer == 'SGD':\n",
    "            scheduler.step()\n",
    "        running_loss += loss.item()\n",
    "    # print(running_loss)\n",
    "    if opt.active_log:\n",
    "        wandb.log({'epoch': epoch ,'train_epoch_loss': running_loss, \n",
    "        'loss': loss.item()\n",
    "        })\n",
    "    if epoch%5==0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                if opt.task in ['binary','multiclass']:\n",
    "                    accuracy, auroc = classification_scores(model, validloader, device, opt.task,vision_dset)\n",
    "                    test_accuracy, test_auroc = classification_scores(model, testloader, device, opt.task,vision_dset)\n",
    "\n",
    "                    print('[EPOCH %d] VALID ACCURACY: %.3f, VALID AUROC: %.3f' %\n",
    "                        (epoch + 1, accuracy,auroc ))\n",
    "                    print('[EPOCH %d] TEST ACCURACY: %.3f, TEST AUROC: %.3f' %\n",
    "                        (epoch + 1, test_accuracy,test_auroc ))\n",
    "                    if opt.active_log:\n",
    "                        wandb.log({'valid_accuracy': accuracy ,'valid_auroc': auroc })     \n",
    "                        wandb.log({'test_accuracy': test_accuracy ,'test_auroc': test_auroc })  \n",
    "                    if opt.task =='multiclass':\n",
    "                        if accuracy > best_valid_accuracy:\n",
    "                            best_valid_accuracy = accuracy\n",
    "                            best_test_auroc = test_auroc\n",
    "                            best_test_accuracy = test_accuracy\n",
    "                            torch.save(model.state_dict(),'%s/bestmodel.pth' % (modelsave_path))\n",
    "                    else:\n",
    "                        if auroc > best_valid_auroc:\n",
    "                            best_valid_auroc = auroc\n",
    "                            best_test_auroc = test_auroc\n",
    "                            best_test_accuracy = test_accuracy               \n",
    "                            torch.save(model.state_dict(),'%s/bestmodel.pth' % (modelsave_path))\n",
    "\n",
    "                else:\n",
    "                    valid_rmse = mean_sq_error(model, validloader, device,vision_dset)    \n",
    "                    test_rmse = mean_sq_error(model, testloader, device,vision_dset)  \n",
    "                    print('[EPOCH %d] VALID RMSE: %.3f' %\n",
    "                        (epoch + 1, valid_rmse ))\n",
    "                    print('[EPOCH %d] TEST RMSE: %.3f' %\n",
    "                        (epoch + 1, test_rmse ))\n",
    "                    if opt.active_log:\n",
    "                        wandb.log({'valid_rmse': valid_rmse ,'test_rmse': test_rmse })     \n",
    "                    if valid_rmse < best_valid_rmse:\n",
    "                        best_valid_rmse = valid_rmse\n",
    "                        best_test_rmse = test_rmse\n",
    "                        torch.save(model.state_dict(),'%s/bestmodel.pth' % (modelsave_path))\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e42dc8-d4d7-4349-8410-dbd2b376458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_parameters = count_parameters(model)\n",
    "print('TOTAL NUMBER OF PARAMS: %d' %(total_parameters))\n",
    "if opt.task =='binary':\n",
    "    print('AUROC on best model:  %.3f' %(best_test_auroc))\n",
    "elif opt.task =='multiclass':\n",
    "    print('Accuracy on best model:  %.3f' %(best_test_accuracy))\n",
    "else:\n",
    "    print('RMSE on best model:  %.3f' %(best_test_rmse))\n",
    "\n",
    "if opt.active_log:\n",
    "    if opt.task == 'regression':\n",
    "        wandb.log({'total_parameters': total_parameters, 'test_rmse_bestep':best_test_rmse , \n",
    "        'cat_dims':len(cat_idxs) , 'con_dims':len(con_idxs) })        \n",
    "    else:\n",
    "        wandb.log({'total_parameters': total_parameters, 'test_auroc_bestep':best_test_auroc , \n",
    "        'test_accuracy_bestep':best_test_accuracy,'cat_dims':len(cat_idxs) , 'con_dims':len(con_idxs) })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
