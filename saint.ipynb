{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed357e88-fc3e-40ea-8e09-381b3ee88af5",
   "metadata": {},
   "source": [
    "# Saint Model Architecture\n",
    "\n",
    "Code from https://github.com/somepago/saint adapted for the dataset and notebook execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b8f1cd4-0b63-421a-b114-1bccb6355c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from types import SimpleNamespace\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from saint.pretrainmodel import SAINT\n",
    "from saint.data_openml import DataSetCatCon, data_split\n",
    "from saint.pretraining import SAINT_pretrain\n",
    "from saint.utils import count_parameters, classification_scores, mean_sq_error, get_scheduler\n",
    "from saint.augmentations import embed_data_mask, add_noise\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "except:\n",
    "    wandb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dd0495f-9fa4-4fd4-bc96-f04b3974d6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is cuda:0.\n"
     ]
    }
   ],
   "source": [
    "opt = SimpleNamespace(**{\n",
    "  'dset_seed': 42,\n",
    "  'dset_id': 'v20,  # dataset version\n",
    "  'run_name': 'train_v3',\n",
    "  'cont_embeddings': 'MLP', # 'MLP', 'Noemb', 'pos_singleMLP'\n",
    "  'attentiontype': 'colrow',  #  'col', 'colrow', 'row', 'justmlp', 'attn', 'attnmlp'\n",
    "  'optimizer': 'AdamW',  # 'AdamW', 'Adam', 'SGD'\n",
    "  'scheduler': 'cosine',  # 'cosine', 'linear'\n",
    "  'embedding_size': 32,\n",
    "  'transformer_depth': 6,\n",
    "  'attention_heads': 8,\n",
    "  'attention_dropout': 0.1,\n",
    "  'ff_dropout': 0.1,\n",
    "  'lr': 0.002,\n",
    "  'epochs': 50,\n",
    "  'batchsize': 2048,\n",
    "  'pretrain': True,  # test with False # TODO\n",
    "  'pretrain_epochs': 15,\n",
    "  'savemodelroot': './bestmodels',\n",
    "  'set_seed': 1,\n",
    "  'active_log': True if wandb else False,  # Weights and Biases API for logging\n",
    "  'pt_tasks': ['contrastive', 'denoising'],  # 'contrastive', 'contrastive_sim', 'denoising'\n",
    "  'pt_aug': [],  # 'mixup', 'cutmix' (list)\n",
    "  'pt_aug_lam': 0.1,\n",
    "  'mixup_lam': 0.3,\n",
    "  'train_noise_type': 'missing', # None, 'missing', 'cutmix'\n",
    "  'train_noise_level': 0.01, \n",
    "  'ssl_samples': None,  # int or None\n",
    "  'pt_projhead_style': 'diff',  # 'diff', 'same', 'nohead'\n",
    "  'nce_temp': 0.7,\n",
    "  'lam0': 0.5,\n",
    "  'lam1': 10,\n",
    "  'lam2': 1,\n",
    "  'lam3': 10,\n",
    "  'final_mlp_style': 'sep', # 'common', 'sep'\n",
    "  'vision_dset': False,\n",
    "  'task': 'regression', 'dtask': 'reg',\n",
    "})\n",
    "modelsave_path = os.path.join(os.getcwd(),opt.savemodelroot,opt.task,str(opt.dset_id),opt.run_name)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device is {device}.\")\n",
    "torch.manual_seed(opt.set_seed)\n",
    "os.makedirs(modelsave_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fcea470-cdb2-491a-b2b4-8230ef3a97eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgabrui\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/gabriel/githubs/prc-challenge-ita/wandb/run-20240917_145641-gpwoacpx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gabrui/saint_v2_robustness/runs/gpwoacpx' target=\"_blank\">regression_missing_0.01_colrow_v8</a></strong> to <a href='https://wandb.ai/gabrui/saint_v2_robustness' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gabrui/saint_v2_robustness' target=\"_blank\">https://wandb.ai/gabrui/saint_v2_robustness</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gabrui/saint_v2_robustness/runs/gpwoacpx' target=\"_blank\">https://wandb.ai/gabrui/saint_v2_robustness/runs/gpwoacpx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if opt.active_log:\n",
    "    try:\n",
    "      import wandb\n",
    "      if opt.train_noise_type is not None and opt.train_noise_level > 0:\n",
    "          wandb.init(project=\"saint_v2_robustness\", group =f'{opt.run_name}_{opt.task}' ,name = f'{opt.task}_{opt.train_noise_type}_{str(opt.train_noise_level)}_{str(opt.attentiontype)}_{str(opt.dset_id)}')\n",
    "      elif opt.ssl_samples is not None:\n",
    "          wandb.init(project=\"saint_v2_ssl\", group = f'{opt.run_name}_{opt.task}' ,name = f'{opt.task}_{str(opt.ssl_samples)}_{str(opt.attentiontype)}_{str(opt.dset_id)}')\n",
    "      else:\n",
    "          raise'wrong config.check the file you are running'\n",
    "      wandb.config.update(opt)\n",
    "    except:\n",
    "      opt.active_log = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03f9b07b-a71f-4ea4-bbd7-7b9c5867c595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_flight_data(dataset_version, dset_seed=42, valid_split=0.1):\n",
    "    df = pd.read_csv(f'data/challenge_set_updated_{dataset_version}.csv')\n",
    "    df_test = pd.read_csv(f'data/submission_set_updated_{dataset_version}.csv')\n",
    "  \n",
    "    train_indices, valid_indices = train_test_split(np.arange(len(df)), test_size=valid_split, random_state=dset_seed)\n",
    "    test_indices = np.arange(len(df), len(df) + len(df_test))\n",
    "    df_combined = pd.concat([df, df_test], ignore_index=True).replace([np.inf, -np.inf], np.nan)\n",
    "    X, y = df_combined.drop(columns=['tow']), df_combined['tow']\n",
    "    # categorical_indicator = df_combined.dtypes.apply(lambda x: not pd.api.types.is_numeric_dtype(x)).to_list()\n",
    "\n",
    "    # categorical_columns = X.columns[list(np.where(np.array(categorical_indicator)==True)[0])].tolist()\n",
    "    # print(categorical_columns)\n",
    "    categorical_columns = ['aircraft_type', \n",
    "             'wtc', \n",
    "             'airline',\n",
    "             'offblock_hour',\n",
    "             'offblock_minute', \n",
    "             'offblock_day_of_week',\n",
    "             'offblock_month',\n",
    "             'offblock_week_of_year', \n",
    "             'offblock_season', \n",
    "             'arrival_hour',\n",
    "             'arrival_minute',\n",
    "             'is_offblock_weekend',\n",
    "             'is_offblock_rush_hour',\n",
    "             'flight_duration_category',                       \n",
    "             'adep_region', \n",
    "             'ades_region', \n",
    "             'same_country_flight',\n",
    "             'same_region_flight',                        \n",
    "             'flight_direction',\n",
    "             'is_intercontinental',\n",
    "             'Manufacturer',\n",
    "             'Model_FAA',\n",
    "             'Physical_Class_Engine',\n",
    "             'FAA_Weight',\n",
    "             'adep_geo_cluster',\n",
    "             'ades_geo_cluster']\n",
    "    cont_columns = list(set(X.columns.tolist()) - set(categorical_columns))\n",
    "    # cat_idxs = list(np.where(np.array(categorical_indicator)==True)[0])\n",
    "    cat_idxs = [df_combined.columns.get_loc(col) for col in categorical_columns]\n",
    "    con_idxs = list(set(range(len(X.columns))) - set(cat_idxs))\n",
    "    for col in categorical_columns:\n",
    "        X[col] = X[col].astype(\"object\")\n",
    "\n",
    "    temp = X.fillna(\"MissingValue\")\n",
    "    nan_mask = temp.ne(\"MissingValue\").astype(int)\n",
    "    \n",
    "    cat_dims = []\n",
    "    for col in categorical_columns:\n",
    "    #     X[col] = X[col].cat.add_categories(\"MissingValue\")\n",
    "        X[col] = X[col].fillna(\"MissingValue\")\n",
    "        l_enc = LabelEncoder() \n",
    "        X[col] = l_enc.fit_transform(X[col].values)\n",
    "        cat_dims.append(len(l_enc.classes_))\n",
    "    for col in cont_columns:\n",
    "    #     X[col].fillna(\"MissingValue\",inplace=True)\n",
    "        X.fillna(X.loc[train_indices, col].mean(), inplace=True)\n",
    "    y = y.values\n",
    "    y_min, y_max = y[train_indices].min()*0.95, y[train_indices].max()*1.05\n",
    "    y = (y - y_min) / (y_max - y_min)\n",
    "    X_train, y_train = data_split(X,y,nan_mask,train_indices)\n",
    "    X_valid, y_valid = data_split(X,y,nan_mask,valid_indices)\n",
    "    X_test, y_test = data_split(X,y,nan_mask,test_indices)\n",
    "\n",
    "    train_mean, train_std = np.array(X_train['data'][:,con_idxs],dtype=np.float32).mean(0), np.array(X_train['data'][:,con_idxs],dtype=np.float32).std(0)\n",
    "    train_std = np.where(train_std < 1e-6, 1e-6, train_std)\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    return cat_dims, cat_idxs, con_idxs, X_train, y_train, X_valid, y_valid, X_test, y_test, train_mean, train_std, y_min, y_max\n",
    "\n",
    "cat_dims, cat_idxs, con_idxs, X_train, y_train, X_valid, y_valid, X_test, y_test, train_mean, train_std, y_min, y_max = load_flight_data(opt.dset_id, opt.dset_seed)\n",
    "continuous_mean_std = np.array([train_mean,train_std]).astype(np.float32)\n",
    "cat_dims = np.append(np.array([1]),np.array(cat_dims)).astype(int) #Appending 1 for CLS token, this is later used to generate embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "124a25d3-8f52-440a-983e-4608989cc3ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dset_seed': 42,\n",
       " 'dset_id': 'v8',\n",
       " 'run_name': 'train_v3',\n",
       " 'cont_embeddings': 'MLP',\n",
       " 'attentiontype': 'colrow',\n",
       " 'optimizer': 'AdamW',\n",
       " 'scheduler': 'cosine',\n",
       " 'embedding_size': 16,\n",
       " 'transformer_depth': 1,\n",
       " 'attention_heads': 4,\n",
       " 'attention_dropout': 0.8,\n",
       " 'ff_dropout': 0.8,\n",
       " 'lr': 0.002,\n",
       " 'epochs': 50,\n",
       " 'batchsize': 2048,\n",
       " 'pretrain': True,\n",
       " 'pretrain_epochs': 15,\n",
       " 'savemodelroot': './bestmodels',\n",
       " 'set_seed': 1,\n",
       " 'active_log': True,\n",
       " 'pt_tasks': ['contrastive', 'denoising'],\n",
       " 'pt_aug': [],\n",
       " 'pt_aug_lam': 0.1,\n",
       " 'mixup_lam': 0.3,\n",
       " 'train_noise_type': 'missing',\n",
       " 'train_noise_level': 0.01,\n",
       " 'ssl_samples': None,\n",
       " 'pt_projhead_style': 'diff',\n",
       " 'nce_temp': 0.7,\n",
       " 'lam0': 0.5,\n",
       " 'lam1': 10,\n",
       " 'lam2': 1,\n",
       " 'lam3': 10,\n",
       " 'final_mlp_style': 'sep',\n",
       " 'vision_dset': False,\n",
       " 'task': 'regression',\n",
       " 'dtask': 'reg'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if opt.attentiontype != 'col':\n",
    "    opt.transformer_depth = 1\n",
    "    opt.attention_heads = 4\n",
    "    opt.attention_dropout = 0.8\n",
    "    opt.embedding_size = 16\n",
    "    if opt.optimizer =='SGD':\n",
    "        opt.ff_dropout = 0.4\n",
    "        opt.lr = 0.01\n",
    "    else:\n",
    "        opt.ff_dropout = 0.8\n",
    "opt.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33e58c71-522d-4f39-9efd-773463c86433",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining begins!\n",
      "Epoch: 0, Running Loss: 4969.757315635681\n",
      "Epoch: 1, Running Loss: 1311.4788708686829\n",
      "Epoch: 2, Running Loss: 992.9838337898254\n",
      "Epoch: 3, Running Loss: 958.7148609161377\n",
      "Epoch: 4, Running Loss: 947.4848208427429\n",
      "Epoch: 5, Running Loss: 940.8644299507141\n",
      "Epoch: 6, Running Loss: 937.3305153846741\n",
      "Epoch: 7, Running Loss: 933.893542766571\n",
      "Epoch: 8, Running Loss: 932.4347143173218\n",
      "Epoch: 9, Running Loss: 931.3027195930481\n",
      "Epoch: 10, Running Loss: 930.0310459136963\n",
      "Epoch: 11, Running Loss: 927.9718689918518\n",
      "Epoch: 12, Running Loss: 927.7993550300598\n",
      "Epoch: 13, Running Loss: 926.7460074424744\n",
      "Epoch: 14, Running Loss: 926.265483379364\n",
      "END OF PRETRAINING!\n"
     ]
    }
   ],
   "source": [
    "train_ds = DataSetCatCon(X_train, y_train, cat_idxs,opt.dtask,continuous_mean_std)\n",
    "trainloader = DataLoader(train_ds, batch_size=opt.batchsize, shuffle=True,num_workers=8)\n",
    "\n",
    "valid_ds = DataSetCatCon(X_valid, y_valid, cat_idxs,opt.dtask, continuous_mean_std)\n",
    "validloader = DataLoader(valid_ds, batch_size=opt.batchsize, shuffle=False,num_workers=8)\n",
    "\n",
    "test_ds = DataSetCatCon(X_test, y_test, cat_idxs,opt.dtask, continuous_mean_std)\n",
    "testloader = DataLoader(test_ds, batch_size=opt.batchsize, shuffle=False,num_workers=8)\n",
    "\n",
    "y_dim = 1 # opt.task 'regression'\n",
    "criterion = nn.MSELoss().to(device)\n",
    "\n",
    "model = SAINT(\n",
    "    categories = tuple(cat_dims), \n",
    "    num_continuous = len(con_idxs),                \n",
    "    dim = opt.embedding_size,                           \n",
    "    dim_out = 1,                       \n",
    "    depth = opt.transformer_depth,                       \n",
    "    heads = opt.attention_heads,                         \n",
    "    attn_dropout = opt.attention_dropout,             \n",
    "    ff_dropout = opt.ff_dropout,                  \n",
    "    mlp_hidden_mults = (4, 2),       \n",
    "    cont_embeddings = opt.cont_embeddings,\n",
    "    attentiontype = opt.attentiontype,\n",
    "    final_mlp_style = opt.final_mlp_style,\n",
    "    y_dim = y_dim\n",
    ")\n",
    "vision_dset = opt.vision_dset\n",
    "\n",
    "# print(count_parameters(model))\n",
    "# import ipdb; ipdb.set_trace()\n",
    "model.to(device)\n",
    "\n",
    "if opt.pretrain:\n",
    "    model = SAINT_pretrain(model, cat_idxs,X_train,y_train, continuous_mean_std, opt,device)\n",
    "\n",
    "if opt.ssl_samples is not None and opt.ssl_samples > 0 :\n",
    "    print('We are in semi-supervised learning case')\n",
    "    train_pts_touse = np.random.choice(X_train['data'].shape[0], opt.ssl_samples)\n",
    "    X_train['data'] = X_train['data'][train_pts_touse,:]\n",
    "    y_train['data'] = y_train['data'][train_pts_touse]\n",
    "    \n",
    "    X_train['mask'] = X_train['mask'][train_pts_touse,:]\n",
    "    train_bsize = min(opt.ssl_samples//4,opt.batchsize)\n",
    "\n",
    "    train_ds = DataSetCatCon(X_train, y_train, cat_idxs,opt.dtask,continuous_mean_std)\n",
    "    trainloader = DataLoader(train_ds, batch_size=train_bsize, shuffle=True,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adcf2981-de13-4a56-bf3d-73c0920df6e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 1] TRAIN RMSE: 1116.950   VALID RMSE: 0.149\n",
      "[EPOCH 2] TRAIN RMSE: 3.734   VALID RMSE: 0.134\n",
      "[EPOCH 3] TRAIN RMSE: 3.121   VALID RMSE: 0.150\n",
      "[EPOCH 4] TRAIN RMSE: 3.042   VALID RMSE: 0.125\n",
      "[EPOCH 5] TRAIN RMSE: 2.435   VALID RMSE: 0.095\n",
      "[EPOCH 6] TRAIN RMSE: 1.991   VALID RMSE: 0.121\n",
      "[EPOCH 7] TRAIN RMSE: 1.864   VALID RMSE: 0.075\n",
      "[EPOCH 8] TRAIN RMSE: 1.402   VALID RMSE: 0.073\n",
      "[EPOCH 9] TRAIN RMSE: 1.125   VALID RMSE: 0.076\n",
      "[EPOCH 10] TRAIN RMSE: 1.135   VALID RMSE: 0.073\n",
      "[EPOCH 11] TRAIN RMSE: 0.693   VALID RMSE: 0.052\n",
      "[EPOCH 12] TRAIN RMSE: 0.579   VALID RMSE: 0.049\n",
      "[EPOCH 13] TRAIN RMSE: 0.519   VALID RMSE: 0.047\n",
      "[EPOCH 14] TRAIN RMSE: 0.495   VALID RMSE: 0.047\n",
      "[EPOCH 15] TRAIN RMSE: 0.487   VALID RMSE: 0.049\n",
      "[EPOCH 16] TRAIN RMSE: 0.437   VALID RMSE: 0.047\n",
      "[EPOCH 17] TRAIN RMSE: 0.454   VALID RMSE: 0.044\n",
      "[EPOCH 18] TRAIN RMSE: 0.404   VALID RMSE: 0.040\n",
      "[EPOCH 19] TRAIN RMSE: 0.346   VALID RMSE: 0.034\n",
      "[EPOCH 20] TRAIN RMSE: 0.262   VALID RMSE: 0.032\n",
      "[EPOCH 21] TRAIN RMSE: 0.216   VALID RMSE: 0.025\n",
      "[EPOCH 22] TRAIN RMSE: 0.211   VALID RMSE: 0.027\n",
      "[EPOCH 23] TRAIN RMSE: 0.186   VALID RMSE: 0.031\n",
      "[EPOCH 24] TRAIN RMSE: 0.174   VALID RMSE: 0.034\n",
      "[EPOCH 25] TRAIN RMSE: 0.142   VALID RMSE: 0.019\n",
      "[EPOCH 26] TRAIN RMSE: 0.126   VALID RMSE: 0.019\n",
      "[EPOCH 27] TRAIN RMSE: 0.104   VALID RMSE: 0.022\n",
      "[EPOCH 28] TRAIN RMSE: 0.109   VALID RMSE: 0.019\n",
      "[EPOCH 29] TRAIN RMSE: 0.097   VALID RMSE: 0.021\n",
      "[EPOCH 30] TRAIN RMSE: 0.087   VALID RMSE: 0.020\n",
      "[EPOCH 31] TRAIN RMSE: 0.084   VALID RMSE: 0.018\n",
      "[EPOCH 32] TRAIN RMSE: 0.081   VALID RMSE: 0.019\n",
      "[EPOCH 33] TRAIN RMSE: 0.073   VALID RMSE: 0.021\n",
      "[EPOCH 34] TRAIN RMSE: 0.071   VALID RMSE: 0.017\n",
      "[EPOCH 35] TRAIN RMSE: 0.067   VALID RMSE: 0.020\n",
      "[EPOCH 36] TRAIN RMSE: 0.072   VALID RMSE: 0.025\n",
      "[EPOCH 37] TRAIN RMSE: 0.063   VALID RMSE: 0.016\n",
      "[EPOCH 38] TRAIN RMSE: 0.055   VALID RMSE: 0.015\n",
      "[EPOCH 39] TRAIN RMSE: 0.056   VALID RMSE: 0.016\n",
      "[EPOCH 40] TRAIN RMSE: 0.053   VALID RMSE: 0.015\n",
      "[EPOCH 41] TRAIN RMSE: 0.048   VALID RMSE: 0.015\n",
      "[EPOCH 42] TRAIN RMSE: 0.050   VALID RMSE: 0.019\n",
      "[EPOCH 43] TRAIN RMSE: 0.049   VALID RMSE: 0.015\n",
      "[EPOCH 44] TRAIN RMSE: 0.050   VALID RMSE: 0.022\n",
      "[EPOCH 45] TRAIN RMSE: 0.047   VALID RMSE: 0.015\n",
      "[EPOCH 46] TRAIN RMSE: 0.047   VALID RMSE: 0.014\n",
      "[EPOCH 47] TRAIN RMSE: 0.042   VALID RMSE: 0.016\n",
      "[EPOCH 48] TRAIN RMSE: 0.043   VALID RMSE: 0.021\n",
      "[EPOCH 49] TRAIN RMSE: 0.046   VALID RMSE: 0.014\n",
      "[EPOCH 50] TRAIN RMSE: 0.042   VALID RMSE: 0.014\n"
     ]
    }
   ],
   "source": [
    "if opt.optimizer == 'SGD':\n",
    "    optimizer = optim.SGD(model.parameters(), lr=opt.lr,\n",
    "                          momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = get_scheduler(opt, optimizer)\n",
    "elif opt.optimizer == 'Adam':\n",
    "    optimizer = optim.Adam(model.parameters(),lr=opt.lr)\n",
    "elif opt.optimizer == 'AdamW':\n",
    "    optimizer = optim.AdamW(model.parameters(),lr=opt.lr)\n",
    "best_valid_auroc = 0\n",
    "best_valid_accuracy = 0\n",
    "best_test_auroc = 0\n",
    "best_test_accuracy = 0\n",
    "best_valid_rmse = 100000\n",
    "patience, current_patience = 10, 0\n",
    "\n",
    "for epoch in range(opt.epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "        # x_categ is the the categorical data, with y appended as last feature. x_cont has continuous data. cat_mask is an array of ones same shape as x_categ except for last column(corresponding to y's) set to 0s. con_mask is an array of ones same shape as x_cont. \n",
    "        x_categ, x_cont, y_gts, cat_mask, con_mask = data[0].to(device), data[1].to(device),data[2].to(device),data[3].to(device),data[4].to(device)\n",
    "        if opt.train_noise_type is not None and opt.train_noise_level>0:\n",
    "            noise_dict = {\n",
    "                'noise_type' : opt.train_noise_type,\n",
    "                'lambda' : opt.train_noise_level\n",
    "            }\n",
    "            if opt.train_noise_type == 'cutmix':\n",
    "                x_categ, x_cont = add_noise(x_categ,x_cont, noise_params = noise_dict)\n",
    "            elif opt.train_noise_type == 'missing':\n",
    "                cat_mask, con_mask = add_noise(cat_mask, con_mask, noise_params = noise_dict)\n",
    "        # We are converting the data to embeddings in the next step\n",
    "        _ , x_categ_enc, x_cont_enc = embed_data_mask(x_categ, x_cont, cat_mask, con_mask,model,vision_dset)           \n",
    "        reps = model.transformer(x_categ_enc, x_cont_enc)\n",
    "        # select only the representations corresponding to y and apply mlp on it in the next step to get the predictions.\n",
    "        y_reps = reps[:,0,:]\n",
    "        \n",
    "        y_outs = model.mlpfory(y_reps)\n",
    "        loss = criterion(y_outs,y_gts) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if opt.optimizer == 'SGD':\n",
    "            scheduler.step()\n",
    "        running_loss += loss.item()\n",
    "    # print(running_loss)\n",
    "    if opt.active_log:\n",
    "        wandb.log({'epoch': epoch ,'train_epoch_loss': running_loss, \n",
    "        'loss': loss.item()\n",
    "        })\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_rmse = mean_sq_error(model, validloader, device,vision_dset)    \n",
    "        # test_rmse = mean_sq_error(model, testloader, device,vision_dset)  \n",
    "        print('[EPOCH %d] TRAIN RMSE: %.3f   VALID RMSE: %.3f' %\n",
    "            (epoch + 1, running_loss, valid_rmse ))\n",
    "        # print('[EPOCH %d] TEST RMSE: %.3f' %\n",
    "        #     (epoch + 1, test_rmse ))\n",
    "        if opt.active_log:\n",
    "            wandb.log({'valid_rmse': valid_rmse}) # ,'test_rmse': test_rmse })     \n",
    "        if valid_rmse < best_valid_rmse:\n",
    "            current_patience = 0\n",
    "            best_valid_rmse = valid_rmse\n",
    "            # best_test_rmse = test_rmse\n",
    "            torch.save(model.state_dict(),'%s/bestmodel.pth' % (modelsave_path))\n",
    "        else:\n",
    "            current_patience += 1\n",
    "    model.train()\n",
    "    if current_patience == patience:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70e42dc8-d4d7-4349-8410-dbd2b376458c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL NUMBER OF PARAMS: 25532070\n",
      "RMSE on best model:  5072.786\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('%s/bestmodel.pth' % (modelsave_path)))\n",
    "total_parameters = count_parameters(model)\n",
    "print('TOTAL NUMBER OF PARAMS: %d' %(total_parameters))\n",
    "print('RMSE on best model:  %.3f' %(best_valid_rmse * (y_max - y_min)))\n",
    "if opt.active_log:\n",
    "    wandb.log({'total_parameters': total_parameters, 'best_valid_rmse':best_valid_rmse, \n",
    "    'cat_dims':len(cat_idxs) , 'con_dims':len(con_idxs) })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c7d13d5-7a4e-42f3-b2f6-2093d9cbdb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(model, dloader, device, vision_dset):\n",
    "    model.eval()\n",
    "    y_test = torch.empty(0).to(device)\n",
    "    y_pred = torch.empty(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dloader, 0):\n",
    "            x_categ, x_cont, y_gts, cat_mask, con_mask = data[0].to(device), data[1].to(device),data[2].to(device),data[3].to(device),data[4].to(device)\n",
    "            _ , x_categ_enc, x_cont_enc = embed_data_mask(x_categ, x_cont, cat_mask, con_mask,model,vision_dset)           \n",
    "            reps = model.transformer(x_categ_enc, x_cont_enc)\n",
    "            y_reps = reps[:,0,:]\n",
    "            y_outs = model.mlpfory(y_reps)\n",
    "            y_test = torch.cat([y_test,y_gts.squeeze().float()],dim=0)\n",
    "            y_pred = torch.cat([y_pred,y_outs],dim=0)\n",
    "        return y_pred.cpu().numpy(), y_test.cpu().numpy()\n",
    "\n",
    "valid_preds, valid_test = get_preds(model, validloader, device, vision_dset)\n",
    "test_preds, _ = get_preds(model, testloader, device, vision_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca160a3b-bdfe-4ade-a670-3ee5be374144",
   "metadata": {},
   "outputs": [],
   "source": [
    "dft0 = pd.read_csv('./data/submission_set.csv')\n",
    "dft0['tow'] = test_preds * (y_max - y_min) + y_min\n",
    "dft0[['flight_id', 'tow']].to_csv('saint.csv', index=False)\n",
    "pd.DataFrame.from_dict({'tow': valid_preds.numpy() * (y_max-y_min) + y_min}).to_csv('saint_val.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
